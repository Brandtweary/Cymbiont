[models]
# Anthropic models: claude-3-5-sonnet-latest, claude-3-5-haiku-latest
# OpenAI models: gpt-4o, gpt-4o-mini, o1-preview
# Huggingface models: llama-3-3-70b-instruct

CHAT_AGENT_MODEL = "claude-3-5-sonnet-latest"  # main chat and tool-calling agent
TAG_EXTRACTION_MODEL = "claude-3-5-haiku-latest"  # bulk document processing for knowledge graph pipeline
PROGRESSIVE_SUMMARY_MODEL = "claude-3-5-haiku-latest"  # summarizes chat history for the chat agent
REVISION_MODEL = "claude-3-5-sonnet-latest"  # used for revise_document tool

[app]
token_logging = false  # show token usage
benchmark = false  # show performance metrics
debug = false  # show debug logs
prompt = false  # show prompts
response = false  # show responses
tool = false  # show tool calls
delete_logs = false  # delete logs on shutdown
file_reset = false  # move processed documents back to input_documents on startup

[shell]
user_name = "user" 
agent_name = "chat_agent" 
agent_activation_mode = "chat" # Options: "continuous" or "chat"

[security]
# Shell access tier (1-5):
# 1: Cymbiont directory read-only
# 2: System-wide read-only
# 3: Project restricted write (can write to data/agent_workspace but not execute)
# 4: Full project write/execute
# 5: Unrestricted

shell_access_tier = 1

[environment]
manage_venv = true  # let Cymbiont manage the Python environment

[local_model_quantization]
# Available values: "4-bit", "8-bit", "none"
llama-3-3-70b-instruct = "none"  # 4-bit quantization recommended for 70B model to fit in a consumer GPU with 48GB VRAM
